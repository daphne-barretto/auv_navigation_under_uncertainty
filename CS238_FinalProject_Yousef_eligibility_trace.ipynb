{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"SX77NLdfunPL"},"outputs":[],"source":["and #@title EligibilityTrace\n","\n","import numpy as np\n","import numpy as np\n","import random\n","\n","def choose_action(state, q_table, epsilon):\n","    if random.uniform(0, 1) < epsilon:\n","        return random.randint(0, q_table.shape[2] - 1)\n","    else:\n","        return np.argmax(q_table[int(state[0]), int(state[1])])\n","\n","def initialize_q_table(grid_size, num_actions):\n","    return np.zeros(grid_size + (num_actions,))\n","\n","def initialize_eligibility_table(grid_size, num_actions):\n","    return np.zeros(grid_size + (num_actions,))\n","\n","def update_q_table(q_table, eligibility_trace, current_state, action, reward, next_state, next_action, alpha, gamma, lambd):\n","    current_q = q_table[current_state + (action,)]\n","    next_q = q_table[next_state + (next_action,)]\n","    td_error = reward + gamma * next_q - current_q\n","\n","    # Update eligibility trace\n","    eligibility_trace[current_state + (action,)] += 1\n","\n","    # # Update Q-values and decay eligibility trace\n","    # for i in range(q_table.shape[0]):\n","    #     for j in range(q_table.shape[1]):\n","    #         for a in range(q_table.shape[2]):\n","    #             q_table[i, j, a] += alpha * td_error * eligibility_trace[i, j, a]\n","    #             eligibility_trace[i, j, a] *= gamma * lambd\n","    # Update Q-values and decay eligibility trace\n","    q_table += alpha * td_error * eligibility_trace\n","    eligibility_trace *= gamma * lambd\n","\n","def derive_policy(q_table):\n","    policy = np.argmax(q_table, axis=2)\n","    return policy\n","\n","def eligibility_trace_method(grid_size, num_actions, alpha, gamma, lambd, transitions):\n","    q_table = initialize_q_table(grid_size, num_actions)\n","    eligibility_trace = initialize_eligibility_table(grid_size, num_actions)\n","\n","    for (i, j, action, reward, m, n) in transitions:\n","        i, j, action, reward, m, n = int(i), int(j), int(action), int(reward), int(m), int(n)\n","        current_state = (i, j)\n","        next_state = (m, n)\n","        next_action = choose_action(next_state, q_table, epsilon)\n","\n","        update_q_table(q_table, eligibility_trace, current_state, action, reward, next_state, next_action, alpha, gamma, lambd)\n","\n","    return derive_policy(q_table)\n","\n","# Parameters\n","grid_size = (n_x, n_y)  # Example grid size\n","num_actions = 4     # 4 actions: up, down, left, right\n","alpha = 0.1         # Learning rate\n","gamma = 0.99        # Discount factor\n","lambd = 0.8         # Eligibility trace decay factor\n","epsilon = 0.1  # Exploration rate\n","\n","# Train the agent with the data\n","import pickle\n","with open('sorted_samples.pkl', 'rb') as file:\n","    data = pickle.load(file)\n","data = [tuple(row) for row in data]\n","\n","# Run SARSA(Î»)\n","policy = eligibility_trace_method(grid_size, num_actions, alpha, gamma, lambd, data)\n"]}]}